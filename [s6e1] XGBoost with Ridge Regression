{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1689.641046,"end_time":"2026-01-26T14:54:17.796604","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-26T14:26:08.155558","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gourabr0y555/s6e1-8-56-xgboost-with-ridge-regression?scriptVersionId=294087985\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Kaggle Playground Series S6E1: Student Exam Score Prediction\n\n**Approach:** XGBoost with Ridge Regression meta-feature stacking\n\n---\n\n## Pipeline Overview\n\n1. Load competition and original datasets\n2. Engineer 34 numeric features (polynomials, logs, interactions, bins)\n3. Train Ridge regression with target encoding â†’ generate OOF meta-feature\n4. Train XGBoost with native categorical support + Ridge predictions as feature\n5. Generate submission via fold averaging","metadata":{"papermill":{"duration":0.003583,"end_time":"2026-01-26T14:26:10.638401","exception":false,"start_time":"2026-01-26T14:26:10.634818","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport warnings\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import TargetEncoder\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:26:10.645034Z","iopub.status.busy":"2026-01-26T14:26:10.6448Z","iopub.status.idle":"2026-01-26T14:26:13.137407Z","shell.execute_reply":"2026-01-26T14:26:13.136813Z"},"papermill":{"duration":2.497833,"end_time":"2026-01-26T14:26:13.139143","exception":false,"start_time":"2026-01-26T14:26:10.64131","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration","metadata":{"papermill":{"duration":0.002583,"end_time":"2026-01-26T14:26:13.144581","exception":false,"start_time":"2026-01-26T14:26:13.141998","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Paths\nTRAIN_PATH = \"/kaggle/input/playground-series-s6e1/train.csv\"\nTEST_PATH = \"/kaggle/input/playground-series-s6e1/test.csv\"\nORIGINAL_PATH = \"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\"\nSUBMISSION_PATH = \"/kaggle/input/playground-series-s6e1/sample_submission.csv\"\n\n# Column names\nTARGET = \"exam_score\"\nID_COL = \"id\"\n\n# Cross-validation\nN_FOLDS = 10\nRANDOM_STATE = 1003","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:26:13.150823Z","iopub.status.busy":"2026-01-26T14:26:13.150496Z","iopub.status.idle":"2026-01-26T14:26:13.154236Z","shell.execute_reply":"2026-01-26T14:26:13.15367Z"},"papermill":{"duration":0.008444,"end_time":"2026-01-26T14:26:13.15559","exception":false,"start_time":"2026-01-26T14:26:13.147146","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Load Data","metadata":{"papermill":{"duration":0.002603,"end_time":"2026-01-26T14:26:13.1608","exception":false,"start_time":"2026-01-26T14:26:13.158197","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\noriginal_df = pd.read_csv(ORIGINAL_PATH)\nsubmission_df = pd.read_csv(SUBMISSION_PATH)\n\nprint(f\"Train:    {train_df.shape}\")\nprint(f\"Test:     {test_df.shape}\")\nprint(f\"Original: {original_df.shape}\")\n\nbase_features = [c for c in train_df.columns if c not in [TARGET, ID_COL]]\ncat_features = train_df.select_dtypes(\"object\").columns.tolist()\n\nprint(f\"\\nBase features: {len(base_features)}\")\nprint(f\"Categorical:   {cat_features}\")","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:26:13.167274Z","iopub.status.busy":"2026-01-26T14:26:13.166823Z","iopub.status.idle":"2026-01-26T14:26:14.652748Z","shell.execute_reply":"2026-01-26T14:26:14.65194Z"},"papermill":{"duration":1.490823,"end_time":"2026-01-26T14:26:14.654282","exception":false,"start_time":"2026-01-26T14:26:13.163459","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Feature Engineering\n\nGenerate 34 engineered features:\n- Polynomial terms (squared)\n- Log and sqrt transforms\n- Pairwise interactions and ratios\n- Ordinal encodings for categorical variables\n- Rule-based binary flags\n- Gap features (distance from ideal values)\n- Binned versions of key numerics","metadata":{"papermill":{"duration":0.002765,"end_time":"2026-01-26T14:26:14.66","exception":false,"start_time":"2026-01-26T14:26:14.657235","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def engineer_features(df, base_cols):\n    \"\"\"Generate engineered features from raw data.\"\"\"\n    out = df.copy()\n    eps = 1e-5\n    \n    # Clipped values for safe transforms\n    study = out['study_hours'].clip(lower=0)\n    attend = out['class_attendance'].clip(lower=0)\n    sleep = out['sleep_hours'].clip(lower=0)\n    \n    # Polynomial features\n    out['study_hours_squared'] = out['study_hours'] ** 2\n    out['class_attendance_squared'] = out['class_attendance'] ** 2\n    out['sleep_hours_squared'] = out['sleep_hours'] ** 2\n    out['age_squared'] = out['age'] ** 2\n    \n    # Log transforms\n    out['log_study_hours'] = np.log1p(study)\n    out['log_class_attendance'] = np.log1p(attend)\n    out['log_sleep_hours'] = np.log1p(sleep)\n    \n    # Sqrt transforms\n    out['sqrt_study_hours'] = np.sqrt(study)\n    out['sqrt_class_attendance'] = np.sqrt(attend)\n    \n    # Interactions\n    out['study_hours_times_attendance'] = out['study_hours'] * out['class_attendance']\n    out['study_hours_times_sleep'] = out['study_hours'] * out['sleep_hours']\n    out['attendance_times_sleep'] = out['class_attendance'] * out['sleep_hours']\n    out['age_times_study_hours'] = out['age'] * out['study_hours']\n    \n    # Ratios\n    out['study_hours_over_sleep'] = out['study_hours'] / (out['sleep_hours'] + eps)\n    out['attendance_over_sleep'] = out['class_attendance'] / (out['sleep_hours'] + eps)\n    out['attendance_over_study'] = out['class_attendance'] / (out['study_hours'] + eps)\n    \n    # Ordinal mappings\n    ordinal_maps = {\n        'sleep_quality': {'poor': 0, 'average': 1, 'good': 2},\n        'facility_rating': {'low': 0, 'medium': 1, 'high': 2},\n        'exam_difficulty': {'easy': 0, 'moderate': 1, 'hard': 2}\n    }\n    for col, mapping in ordinal_maps.items():\n        out[f'{col}_numeric'] = out[col].map(mapping).fillna(1).astype(int)\n    \n    # Ordinal interactions\n    out['study_hours_times_sleep_quality'] = out['study_hours'] * out['sleep_quality_numeric']\n    out['attendance_times_facility'] = out['class_attendance'] * out['facility_rating_numeric']\n    out['sleep_hours_times_difficulty'] = out['sleep_hours'] * out['exam_difficulty_numeric']\n    out['facility_x_sleepq'] = out['facility_rating_numeric'] * out['sleep_quality_numeric']\n    out['difficulty_x_facility'] = out['exam_difficulty_numeric'] * out['facility_rating_numeric']\n    \n    # Rule-based flags\n    out['high_att_high_study'] = ((out['class_attendance'] >= 90) & (out['study_hours'] >= 6)).astype(int)\n    out['ideal_sleep_flag'] = ((out['sleep_hours'] >= 7) & (out['sleep_hours'] <= 9)).astype(int)\n    out['high_study_flag'] = (out['study_hours'] >= 7).astype(int)\n    \n    # Composite efficiency\n    out['efficiency'] = (out['study_hours'] * out['class_attendance']) / (out['sleep_hours'] + 1)\n    \n    # Gap features\n    out['sleep_gap_8'] = (out['sleep_hours'] - 8.0).abs()\n    out['attendance_gap_100'] = (out['class_attendance'] - 100.0).abs()\n    \n    # Binned features\n    out['study_bin_num'] = pd.cut(out['study_hours'], bins=5, labels=False).astype(int)\n    out['attendance_bin_num'] = pd.cut(out['class_attendance'], bins=5, labels=False).astype(int)\n    out['sleep_bin_num'] = pd.cut(out['sleep_hours'], bins=5, labels=False).astype(int)\n    out['age_bin_num'] = pd.cut(out['age'], bins=5, labels=False).astype(int)\n    \n    engineered_cols = [\n        'study_hours_squared', 'class_attendance_squared', 'sleep_hours_squared', 'age_squared',\n        'log_study_hours', 'log_class_attendance', 'log_sleep_hours',\n        'sqrt_study_hours', 'sqrt_class_attendance',\n        'study_hours_times_attendance', 'study_hours_times_sleep', 'attendance_times_sleep',\n        'age_times_study_hours',\n        'study_hours_over_sleep', 'attendance_over_sleep', 'attendance_over_study',\n        'sleep_quality_numeric', 'facility_rating_numeric', 'exam_difficulty_numeric',\n        'study_hours_times_sleep_quality', 'attendance_times_facility', 'sleep_hours_times_difficulty',\n        'facility_x_sleepq', 'difficulty_x_facility',\n        'high_att_high_study', 'ideal_sleep_flag', 'high_study_flag',\n        'efficiency',\n        'sleep_gap_8', 'attendance_gap_100',\n        'study_bin_num', 'attendance_bin_num', 'sleep_bin_num', 'age_bin_num'\n    ]\n    \n    return out[base_cols + engineered_cols], engineered_cols","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:26:14.666692Z","iopub.status.busy":"2026-01-26T14:26:14.666412Z","iopub.status.idle":"2026-01-26T14:26:14.679461Z","shell.execute_reply":"2026-01-26T14:26:14.678772Z"},"papermill":{"duration":0.018075,"end_time":"2026-01-26T14:26:14.680839","exception":false,"start_time":"2026-01-26T14:26:14.662764","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply feature engineering\nX_train, engineered_cols = engineer_features(train_df, base_features)\nX_test, _ = engineer_features(test_df, base_features)\nX_orig, _ = engineer_features(original_df, base_features)\n\ny_train = train_df[TARGET].reset_index(drop=True)\ny_orig = original_df[TARGET].reset_index(drop=True)\n\n# Combine for consistent processing\nfull_data = pd.concat([X_train, X_test, X_orig], axis=0, ignore_index=True)\nfor col in engineered_cols:\n    full_data[col] = full_data[col].astype(float)\n\nn_train, n_test = len(train_df), len(test_df)\nX = full_data.iloc[:n_train].copy()\nX_test = full_data.iloc[n_train:n_train + n_test].copy()\nX_original = full_data.iloc[n_train + n_test:].copy()\n\nprint(f\"Engineered features: {len(engineered_cols)}\")\nprint(f\"Total features:      {X.shape[1]} (11 base + {len(engineered_cols)} engineered)\")","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:26:14.687067Z","iopub.status.busy":"2026-01-26T14:26:14.686842Z","iopub.status.idle":"2026-01-26T14:26:16.067766Z","shell.execute_reply":"2026-01-26T14:26:16.066793Z"},"papermill":{"duration":1.385792,"end_time":"2026-01-26T14:26:16.069234","exception":false,"start_time":"2026-01-26T14:26:14.683442","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Ridge Regression Meta-Feature\n\nTrain Ridge with target-encoded categoricals. OOF predictions become an additional feature for XGBoost.","metadata":{"papermill":{"duration":0.002877,"end_time":"2026-01-26T14:26:16.075224","exception":false,"start_time":"2026-01-26T14:26:16.072347","status":"completed"},"tags":[]}},{"cell_type":"code","source":"kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\noof_ridge = np.zeros(len(X))\ntest_preds_ridge = np.zeros((len(X_test), N_FOLDS))\norig_preds_ridge = np.zeros(len(X_original))\n\nridge_alphas = np.logspace(-3, 3, 20)\n\nprint(\"Training Ridge Regression\")\nprint(\"-\" * 40)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y_train), 1):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # Augment with original data\n    X_tr_aug = pd.concat([X_tr, X_original], axis=0)\n    y_tr_aug = pd.concat([y_tr, y_orig], axis=0)\n    \n    # Target encode categoricals\n    encoder = TargetEncoder(smooth='auto', target_type='continuous')\n    X_tr_enc = X_tr_aug.copy()\n    X_val_enc = X_val.copy()\n    X_test_enc = X_test.copy()\n    \n    X_tr_enc[cat_features] = encoder.fit_transform(X_tr_aug[cat_features], y_tr_aug)\n    X_val_enc[cat_features] = encoder.transform(X_val[cat_features])\n    X_test_enc[cat_features] = encoder.transform(X_test[cat_features])\n    \n    # Fit Ridge\n    ridge = RidgeCV(alphas=ridge_alphas, cv=5, scoring='neg_root_mean_squared_error')\n    ridge.fit(X_tr_enc, y_tr_aug.values.ravel())\n    \n    # Predictions (clipped to valid range)\n    oof_ridge[val_idx] = np.clip(ridge.predict(X_val_enc), 0, 100)\n    test_preds_ridge[:, fold - 1] = np.clip(ridge.predict(X_test_enc), 0, 100)\n    orig_preds_ridge += np.clip(ridge.predict(X_tr_enc.iloc[-len(X_original):]), 0, 100) / N_FOLDS\n    \n    rmse = np.sqrt(mean_squared_error(y_val, oof_ridge[val_idx]))\n    print(f\"Fold {fold:2d} | RMSE: {rmse:.6f}\")\n\nridge_oof_rmse = np.sqrt(mean_squared_error(y_train, oof_ridge))\nprint(f\"\\nRidge OOF RMSE: {ridge_oof_rmse:.6f}\")","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:26:16.082321Z","iopub.status.busy":"2026-01-26T14:26:16.08188Z","iopub.status.idle":"2026-01-26T14:32:44.67287Z","shell.execute_reply":"2026-01-26T14:32:44.672055Z"},"papermill":{"duration":388.600447,"end_time":"2026-01-26T14:32:44.678581","exception":false,"start_time":"2026-01-26T14:26:16.078134","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Prepare XGBoost Data\n\nAdd Ridge predictions as meta-feature and convert categoricals to native categorical dtype.","metadata":{"papermill":{"duration":0.005167,"end_time":"2026-01-26T14:32:44.688954","exception":false,"start_time":"2026-01-26T14:32:44.683787","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Convert categoricals for XGBoost native handling\nfor col in base_features:\n    full_data[col] = full_data[col].astype(str).astype(\"category\")\nfor col in engineered_cols:\n    full_data[col] = full_data[col].astype(float)\n\nX_xgb = full_data.iloc[:n_train].copy()\nX_test_xgb = full_data.iloc[n_train:n_train + n_test].copy()\nX_orig_xgb = full_data.iloc[n_train + n_test:].copy()\n\n# Add Ridge meta-feature\nX_xgb['ridge_pred'] = oof_ridge\nX_test_xgb['ridge_pred'] = test_preds_ridge.mean(axis=1)\nX_orig_xgb['ridge_pred'] = orig_preds_ridge\n\nprint(f\"Final feature count: {X_xgb.shape[1]} (including Ridge meta-feature)\")","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:32:44.700276Z","iopub.status.busy":"2026-01-26T14:32:44.700007Z","iopub.status.idle":"2026-01-26T14:32:47.035652Z","shell.execute_reply":"2026-01-26T14:32:47.034859Z"},"papermill":{"duration":2.343225,"end_time":"2026-01-26T14:32:47.037238","exception":false,"start_time":"2026-01-26T14:32:44.694013","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Optuna Hyperparameter Tuning (GPU-Accelerated)\n\nUse Optuna to find optimal XGBoost hyperparameters with GPU acceleration:\n- Tree-structured Parzen Estimator (TPE) sampler\n- Median pruning for early stopping of unpromising trials\n- GPU-accelerated XGBoost training with `device='cuda'`","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.pruners import MedianPruner\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optuna_objective(trial):\n    \"\"\"\n    Optuna objective function for XGBoost hyperparameter tuning.\n    Uses 3-fold CV for faster tuning, GPU acceleration.\n    \"\"\"\n    params = {\n        'n_estimators': 10000,  # Use early stopping\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n        'max_depth': trial.suggest_int('max_depth', 4, 12),\n        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.9),\n        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.3, 0.9),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 20.0, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 5.0, log=True),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n        'tree_method': 'hist',\n        'device': 'cuda',  # GPU acceleration\n        'enable_categorical': True,\n        'eval_metric': 'rmse',\n        'early_stopping_rounds': 50,\n        'random_state': RANDOM_STATE,\n    }\n    \n    # Use 3-fold CV for faster tuning\n    cv_kf = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n    cv_scores = []\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(cv_kf.split(X_xgb, y_train)):\n        X_tr, X_val = X_xgb.iloc[train_idx], X_xgb.iloc[val_idx]\n        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n        \n        # Augment with original data\n        X_tr_aug = pd.concat([X_tr, X_orig_xgb], axis=0)\n        y_tr_aug = pd.concat([y_tr, y_orig], axis=0)\n        \n        model = xgb.XGBRegressor(**params)\n        model.fit(\n            X_tr_aug, y_tr_aug,\n            eval_set=[(X_val, y_val)],\n            verbose=False\n        )\n        \n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        cv_scores.append(rmse)\n        \n        # Report intermediate value for pruning\n        trial.report(np.mean(cv_scores), fold_idx)\n        \n        # Prune if unpromising\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n    \n    return np.mean(cv_scores)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nN_OPTUNA_TRIALS = 50  # Adjust based on time budget\nOPTUNA_TIMEOUT = 3600  # 1 hour timeout (optional)\n\nprint(\"Starting Optuna Hyperparameter Optimization\")\nprint(\"=\" * 50)\nprint(f\"Trials: {N_OPTUNA_TRIALS}\")\nprint(f\"Using GPU acceleration (device='cuda')\")\nprint(\"=\" * 50)\n\n# Create study with TPE sampler and median pruner\nstudy = optuna.create_study(\n    direction='minimize',\n    sampler=TPESampler(seed=RANDOM_STATE),\n    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n)\n\n# Run optimization\nstudy.optimize(\n    optuna_objective,\n    n_trials=N_OPTUNA_TRIALS,\n    timeout=OPTUNA_TIMEOUT,\n    show_progress_bar=True\n)\n\nprint(f\"\\nOptimization Complete!\")\nprint(f\"Best trial RMSE: {study.best_trial.value:.5f}\")\nprint(f\"Best parameters:\")\nfor key, value in study.best_trial.params.items():\n    print(f\"  {key}: {value}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build final XGBoost parameters from Optuna results\nbest_params = study.best_trial.params\n\nxgb_params = {\n    'n_estimators': 20000,  # High value with early stopping\n    'learning_rate': best_params['learning_rate'],\n    'max_depth': best_params['max_depth'],\n    'subsample': best_params['subsample'],\n    'colsample_bytree': best_params['colsample_bytree'],\n    'colsample_bynode': best_params['colsample_bynode'],\n    'reg_lambda': best_params['reg_lambda'],\n    'reg_alpha': best_params['reg_alpha'],\n    'min_child_weight': best_params['min_child_weight'],\n    'gamma': best_params['gamma'],\n    'tree_method': 'hist',\n    'device': 'cuda',  # GPU acceleration\n    'enable_categorical': True,\n    'eval_metric': 'rmse',\n    'early_stopping_rounds': 100,\n    'random_state': RANDOM_STATE,\n}\n\nprint(\"Final XGBoost Parameters (Optuna-tuned):\")\nprint(\"-\" * 40)\nfor k, v in xgb_params.items():\n    print(f\"{k}: {v}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Train XGBoost with Optuna-tuned Parameters","metadata":{}},{"cell_type":"code","source":"test_preds_xgb = []\noof_xgb = np.zeros(len(X_xgb))\n\nprint(\"Training XGBoost\")\nprint(\"-\" * 40)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_xgb, y_train), 1):\n    print(f\"\\nFold {fold}/{N_FOLDS}\")\n    \n    X_tr, X_val = X_xgb.iloc[train_idx], X_xgb.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # Augment with original data\n    X_tr_aug = pd.concat([X_tr, X_orig_xgb], axis=0)\n    y_tr_aug = pd.concat([y_tr, y_orig], axis=0)\n    \n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(\n        X_tr_aug, y_tr_aug,\n        eval_set=[(X_val, y_val)],\n        verbose=1000\n    )\n    \n    oof_xgb[val_idx] = model.predict(X_val)\n    test_preds_xgb.append(model.predict(X_test_xgb))\n    \n    rmse = np.sqrt(mean_squared_error(y_val, oof_xgb[val_idx]))\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\nxgb_oof_rmse = np.sqrt(mean_squared_error(y_train, oof_xgb))\nprint(f\"\\nXGBoost OOF RMSE: {xgb_oof_rmse:.5f}\")","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:32:47.063495Z","iopub.status.busy":"2026-01-26T14:32:47.063051Z","iopub.status.idle":"2026-01-26T14:54:15.653929Z","shell.execute_reply":"2026-01-26T14:54:15.653115Z"},"papermill":{"duration":1288.59612,"end_time":"2026-01-26T14:54:15.655399","exception":false,"start_time":"2026-01-26T14:32:47.059279","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Results Summary","metadata":{"papermill":{"duration":0.004874,"end_time":"2026-01-26T14:54:15.665385","exception":false,"start_time":"2026-01-26T14:54:15.660511","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"Model Performance\")\nprint(\"-\" * 40)\nprint(f\"Ridge OOF RMSE:   {ridge_oof_rmse:.6f}\")\nprint(f\"XGBoost OOF RMSE: {xgb_oof_rmse:.5f}\")\n\nprint(f\"\\nFeature Summary\")\nprint(\"-\" * 40)\nprint(f\"Base features:       {len(base_features)}\")\nprint(f\"Engineered features: {len(engineered_cols)}\")\nprint(f\"Meta-feature:        1\")\nprint(f\"Total:               {X_xgb.shape[1]}\")","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:54:15.676284Z","iopub.status.busy":"2026-01-26T14:54:15.675817Z","iopub.status.idle":"2026-01-26T14:54:15.680788Z","shell.execute_reply":"2026-01-26T14:54:15.680005Z"},"papermill":{"duration":0.012003,"end_time":"2026-01-26T14:54:15.682158","exception":false,"start_time":"2026-01-26T14:54:15.670155","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Save Outputs","metadata":{"papermill":{"duration":0.004687,"end_time":"2026-01-26T14:54:15.691613","exception":false,"start_time":"2026-01-26T14:54:15.686926","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# OOF predictions\noof_df = pd.DataFrame({ID_COL: train_df[ID_COL], TARGET: oof_xgb})\noof_df.to_csv(\"xgb_oof_optimized.csv\", index=False)\n\n# Submission\nsubmission = submission_df.copy()\nsubmission[TARGET] = np.mean(test_preds_xgb, axis=0)\nsubmission.to_csv(\"submission_optimized.csv\", index=False)\n\nprint(\"Saved:\")\nprint(\"  submission_optimized.csv\")\nprint(\"  xgb_oof_optimized.csv\")","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:54:15.70206Z","iopub.status.busy":"2026-01-26T14:54:15.701847Z","iopub.status.idle":"2026-01-26T14:54:17.108466Z","shell.execute_reply":"2026-01-26T14:54:17.107668Z"},"papermill":{"duration":1.413669,"end_time":"2026-01-26T14:54:17.110047","exception":false,"start_time":"2026-01-26T14:54:15.696378","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Feature Importance","metadata":{"papermill":{"duration":0.004978,"end_time":"2026-01-26T14:54:17.120797","exception":false,"start_time":"2026-01-26T14:54:17.115819","status":"completed"},"tags":[]}},{"cell_type":"code","source":"importance_scores = model.get_booster().get_score(importance_type=\"gain\")\n\nimportance_df = pd.DataFrame({\n    \"feature\": list(importance_scores.keys()),\n    \"importance\": list(importance_scores.values())\n}).sort_values(\"importance\", ascending=False)\n\nimportance_df['importance_pct'] = 100 * importance_df['importance'] / importance_df['importance'].sum()\nimportance_df.to_csv(\"feature_importance_optimized.csv\", index=False)\n\nprint(\"Top 10 Features by Gain\")\nprint(\"-\" * 50)\nprint(importance_df.head(10)[['feature', 'importance_pct']].to_string(index=False))","metadata":{"execution":{"iopub.execute_input":"2026-01-26T14:54:17.131626Z","iopub.status.busy":"2026-01-26T14:54:17.131344Z","iopub.status.idle":"2026-01-26T14:54:17.172865Z","shell.execute_reply":"2026-01-26T14:54:17.172033Z"},"papermill":{"duration":0.048579,"end_time":"2026-01-26T14:54:17.174248","exception":false,"start_time":"2026-01-26T14:54:17.125669","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}